# Postcards from my Jungle

## Model Training

Postcards from my Jungle is a generative audiovisual web project that explores camouflaged animals, both in images and sound, through AI-driven recognition and regeneration, sort of as an experiment to see if AI would be able to identify these hidden creatures in the images and reconstruct them into images in which we can retrace the animals. To train my image generation models, I used the **Jugga 386** dataset of camouflaged animals, available on [Hugging Face](https://huggingface.co/datasets/jubba/camouflaged_animals).  The training process was computationally demanding: I processed images at a 128x128 resolution, running 32 epochs with a batch size of 16. This resulted in high memory consumption, often exceeding 30 GB of GPU RAM—far beyond the 15 GB limit of Google Colab's free tier. To accommodate these needs, I transitioned to Google Colab Pro, which made the operation possible. The full training process took approximately 2.5 hours to complete.

## Image and Sound Generation

After gathering a fully trained model, I generated 50 images based on the training, all exported into individual png files and displayed together in a grid in another png file. The images came out in a 128x128 resolution, the same they were trained on. This operation was quite long as well, as it was making use of a complex model to do the operation. The sound however, was much lighter in the generation process. The script to do this uses Stable Audio, an AI-based generative audio model, to create immersive soundscapes inspired by the camouflaged animals the environments presented in the images of the dataset. My goal was to represent as much as I could the images my model was trained on with sound. I prompted the following: **Generate a soundscape of a jungle, rich with nature sounds. The atmosphere should be dense with the rustling of trees, leaves, and branches swaying. Camouflaged animals blend into the environment, through their sounds—soft bird calls, distant owl hoots, occasional gecko chirps, the rhythmic croaking of frogs, and the delicate flapping of butterfly wings**. The script then generates multiple waveforms using a deep learning model on the Google Colab GPU, and saves them as .wav files for playback.

## Insights Gained and Possible Improvements

This experiment revealed that the AI struggled to regenerate camouflaged animals in images or even create coherent natural environments. The results were grainy, swirling visuals where only nature-like colors were recognizable. While I anticipated that detecting and reconstructing hidden animals would be challenging, the primary issue seemed to be insufficient training. Running 32 epochs over 2.5 hours initially seemed adequate, but the model clearly required more. To improve, I would increase the epochs to, let's say, at least 200, and optimize performance with a higher batch size and a higher image resolution for training. These adjustments could undoubtably lead to noticeable improvements, but that cannot be done until a more powerful GPU is secured, as Google Colab, even with the Pro package, has its limitations.
